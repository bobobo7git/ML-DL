{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9e7fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find platform independent libraries <prefix>\n"
     ]
    }
   ],
   "source": [
    "!python -m venv venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "375b69f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (2.7.0+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (2.7.0+cu118)\n",
      "Requirement already satisfied: librosa in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: standard-aifc in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: standard-sunau in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: standard-chunk in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from standard-aifc->librosa) (3.13.0)\n",
      "Requirement already satisfied: audioop-lts in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from standard-aifc->librosa) (0.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find platform independent libraries <prefix>\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchaudio librosa numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72aa14d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def find_wav_files(base_dir, is_train:bool, per_class=100):\n",
    "    sub_root = '1.Training' if is_train else '2.Validation'\n",
    "\n",
    "    wav_files = []\n",
    "    emotion_dir = os.path.join(\n",
    "        base_dir, \n",
    "        '01.데이터', \n",
    "        sub_root,\n",
    "        '원천데이터',\n",
    "        ('T' if is_train else 'V') + 'S1',\n",
    "        ('T' if is_train else 'V') + 'S1',\n",
    "        '1.감정'\n",
    "    )\n",
    "    \n",
    "    for emo in os.listdir(emotion_dir):\n",
    "        emo_path = os.path.join(emotion_dir, emo)\n",
    "        if not os.path.isdir(emo_path): continue\n",
    "        for sub_folder in os.listdir(emo_path):\n",
    "            sub_emo_path = os.path.join(emo_path, sub_folder)\n",
    "            if not os.path.isdir(sub_emo_path): continue\n",
    "\n",
    "            wav_list = [\n",
    "                os.path.join(sub_emo_path, f)\n",
    "                for f in os.listdir(sub_emo_path) if f.endswith('.wav')\n",
    "            ]\n",
    "            if len(wav_list) == 0:\n",
    "                continue\n",
    "            \n",
    "            # sampled = random.sample(wav_list, min(per_class, len(wav_list)))\n",
    "            # wav_files.extend(sampled)\n",
    "            wav_files.extend(wav_list)\n",
    "    return wav_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "601f744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = r'C:\\Users\\SSAFY\\Downloads\\015.감성 및 발화 스타일별 음성합성 데이터'\n",
    "wav_list = find_wav_files(BASE_DIR, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08a418f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "453924"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wav_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ef5847c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\SSAFY\\\\Downloads\\\\015.감성 및 발화 스타일별 음성합성 데이터\\\\01.데이터\\\\1.Training\\\\라벨링데이터\\\\TL1\\\\TL1\\\\1.감정\\\\1.기쁨\\\\0001_G1A3E1S0C0_PSB\\\\0001_G1A3E1S0C0_PSB_000001.json',\n",
       " 'C:\\\\Users\\\\SSAFY\\\\Downloads\\\\015.감성 및 발화 스타일별 음성합성 데이터\\\\01.데이터\\\\1.Training\\\\라벨링데이터\\\\TL1\\\\TL1\\\\1.감정\\\\1.기쁨\\\\0001_G1A3E1S0C0_PSB\\\\0001_G1A3E1S0C0_PSB_000002.json',\n",
       " 'C:\\\\Users\\\\SSAFY\\\\Downloads\\\\015.감성 및 발화 스타일별 음성합성 데이터\\\\01.데이터\\\\1.Training\\\\라벨링데이터\\\\TL1\\\\TL1\\\\1.감정\\\\1.기쁨\\\\0001_G1A3E1S0C0_PSB\\\\0001_G1A3E1S0C0_PSB_000003.json',\n",
       " 'C:\\\\Users\\\\SSAFY\\\\Downloads\\\\015.감성 및 발화 스타일별 음성합성 데이터\\\\01.데이터\\\\1.Training\\\\라벨링데이터\\\\TL1\\\\TL1\\\\1.감정\\\\1.기쁨\\\\0001_G1A3E1S0C0_PSB\\\\0001_G1A3E1S0C0_PSB_000004.json',\n",
       " 'C:\\\\Users\\\\SSAFY\\\\Downloads\\\\015.감성 및 발화 스타일별 음성합성 데이터\\\\01.데이터\\\\1.Training\\\\라벨링데이터\\\\TL1\\\\TL1\\\\1.감정\\\\1.기쁨\\\\0001_G1A3E1S0C0_PSB\\\\0001_G1A3E1S0C0_PSB_000005.json']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def match_label_file(wav_path, base_dir:str, is_train:bool):\n",
    "    wav_tag = 'TS' if is_train else 'VS' \n",
    "    json_tag = 'TL' if is_train else 'VL' \n",
    "    label_path = wav_path.replace(\"원천데이터\", \"라벨링데이터\")\n",
    "    label_path = label_path.replace(f'{wav_tag}1', f'{json_tag}1')\n",
    "    label_path = label_path.replace(\".wav\", \".json\")\n",
    "    return label_path\n",
    "\n",
    "label_path_list = [match_label_file(wav, BASE_DIR, True) for wav in wav_list if os.path.exists(wav)]\n",
    "label_path_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14dbef2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Happy'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "def get_emotion_label(json_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    # pprint(data['화자정보'])\n",
    "    emotion = data['화자정보']['Emotion']\n",
    "    return emotion\n",
    "\n",
    "get_emotion_label(label_path_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0adea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dict()\n",
    "for label in label_path_list:\n",
    "    emo = get_emotion_label(label)\n",
    "    if temp.get(emo, False): temp[emo] += 1\n",
    "    else: temp[emo] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0a6cfed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Happy': 330,\n",
       " 'Sad': 320,\n",
       " 'Angry': 320,\n",
       " 'Anxious': 320,\n",
       " 'Hurt': 320,\n",
       " 'Embarrassed': 320,\n",
       " 'Neutrality': 320}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f25ef197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Happy': 0, 'Sad': 1, 'Angry': 2, 'Anxious': 3, 'Hurt': 4, 'Embarrassed': 5, 'Neutrality': 6} {0: 'Happy', 1: 'Sad', 2: 'Angry', 3: 'Anxious', 4: 'Hurt', 5: 'Embarrassed', 6: 'Neutrality'}\n"
     ]
    }
   ],
   "source": [
    "_id = 0\n",
    "label2id = dict()\n",
    "id2label = dict()\n",
    "for emo in temp.keys():\n",
    "    label2id[emo] = _id\n",
    "    id2label[_id] = emo\n",
    "    _id += 1\n",
    "    \n",
    "del _id\n",
    "print(label2id, id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e3ba305",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'Happy': 0, 'Sad': 1, 'Angry': 2, 'Anxious': 3, 'Hurt': 4, 'Embarrassed': 5, 'Neutrality': 6}\n",
    "id2label = {0: 'Happy', 1: 'Sad', 2: 'Angry', 3: 'Anxious', 4: 'Hurt', 5: 'Embarrassed', 6: 'Neutrality'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = list(label2id[e] for e in map(get_emotion_label, label_path_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aaeec301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa \n",
    "\n",
    "def load_audio(path, target_sr=16000):\n",
    "    y, sr = librosa.load(path, sr=target_sr)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eabb7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_files, test_files, train_labels, test_labels = train_test_split(\n",
    "    wav_list, label_list, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4d5e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, audio_paths, labels, target_sr=16000):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.labels = labels\n",
    "        self.target_sr = target_sr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav = load_audio(self.audio_paths[idx], self.target_sr)\n",
    "        label = self.labels[idx]\n",
    "        return {\"input_values\": wav, \"labels\": label}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd4c0162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_mfcc(\n",
    "    audio_path: str,\n",
    "    sr: int = 16000,\n",
    "    n_fft: int = 512,\n",
    "    hop_length: int = 160,\n",
    "    n_mfcc: int = 20\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    오디오에서 MFCC + 델타·이중델타를 뽑아서 (frames, n_mfcc*3) 형태로 반환\n",
    "    \"\"\"\n",
    "    # 1) 로드 & 프리엠퍼시스\n",
    "    y, _ = librosa.load(audio_path, sr=sr)\n",
    "    y = np.append(y[0], y[1:] - 0.97 * y[:-1])\n",
    "\n",
    "    # 2) 멜 스펙트로그램 (파워 스펙트럼에 멜 필터뱅크 적용)\n",
    "    S = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))**2\n",
    "    mel_basis = librosa.filters.mel(sr, n_fft, n_mels=26)\n",
    "    mel_S = mel_basis @ S\n",
    "\n",
    "    # 3) 로그 스케일  \n",
    "    log_mel_S = np.log(mel_S + 1e-8)\n",
    "\n",
    "    # 4) MFCC (DCT 적용)  \n",
    "    mfcc = librosa.feature.mfcc(S=log_mel_S, n_mfcc=n_mfcc)\n",
    "\n",
    "    # 5) 델타·이중델타 추가  \n",
    "    mfcc_delta   = librosa.feature.delta(mfcc, order=1)\n",
    "    mfcc_delta2  = librosa.feature.delta(mfcc, order=2)\n",
    "\n",
    "    # 반환 형태: (frames, n_mfcc * 3)\n",
    "    return np.concatenate([mfcc, mfcc_delta, mfcc_delta2], axis=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bd3b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def create_feature_filter(\n",
    "    per_emotion_importances: List[Dict[str, float]],\n",
    "    top_k: int = 10\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    per_emotion_importances:  \n",
    "      [ {'MFCC_2':0.15, 'MFCC_8':0.12, …},    # 분노\n",
    "        {'MFCC_1':0.11, 'Mel_4':0.09, …},    # 기쁨\n",
    "        …                                 # 총 7개 감정\n",
    "      ]\n",
    "    반환: 공통 상위 top_k 특징의 평균 가중치 딕셔너리\n",
    "    \"\"\"\n",
    "    # 1) 감정별로 상위 top_k 뽑기\n",
    "    top_features = []\n",
    "    for imp in per_emotion_importances:\n",
    "        sorted_feats = sorted(imp.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        top_features.append({feat: weight for feat, weight in sorted_feats})\n",
    "\n",
    "    # 2) 모든 감정에서 등장한 가중치 합산\n",
    "    agg_weights: Dict[str, float] = {}\n",
    "    for feats in top_features:\n",
    "        for feat, w in feats.items():\n",
    "            agg_weights[feat] = agg_weights.get(feat, 0.0) + w\n",
    "\n",
    "    # 3) 합산 가중치 기준으로 다시 상위 top_k 선택\n",
    "    common = dict(sorted(agg_weights.items(), key=lambda x: x[1], reverse=True)[:top_k])\n",
    "    return common\n",
    "\n",
    "def compute_emotion_index(\n",
    "    features: Dict[str, Any],\n",
    "    filter_weights: Dict[str, float]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    features: {'MFCC_2': value, 'MFCC_8': value, …}\n",
    "    filter_weights: {'MFCC_2': w2, 'MFCC_8': w8, …}\n",
    "    수식 (3) 감정 지수:  Σ_i (feature_i * weight_i)\n",
    "    \"\"\"\n",
    "    return sum(features.get(f, 0.0) * w for f, w in filter_weights.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9874ed23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting MFCC:   5%|▌         | 23354/453924 [04:35<1:24:30, 84.92it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m wav_path \u001b[38;5;129;01min\u001b[39;00m tqdm(wav_list, desc=\u001b[33m\"\u001b[39m\u001b[33mExtracting MFCC\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         mfcc = \u001b[43mextract_mfcc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m            \u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwav_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m            \u001b[49m\u001b[43msr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m160\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_mfcc\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# returns shape (frames, n_mfcc*3)\u001b[39;00m\n\u001b[32m     17\u001b[39m         \u001b[38;5;66;03m# 파일명 기준으로 저장\u001b[39;00m\n\u001b[32m     18\u001b[39m         fname = os.path.splitext(os.path.basename(wav_path))[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mextract_mfcc\u001b[39m\u001b[34m(audio_path, sr, n_fft, hop_length, n_mfcc)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m오디오에서 MFCC + 델타·이중델타를 뽑아서 (frames, n_mfcc*3) 형태로 반환\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 1) 로드 & 프리엠퍼시스\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m y, _ = \u001b[43mlibrosa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m=\u001b[49m\u001b[43msr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m y = np.append(y[\u001b[32m0\u001b[39m], y[\u001b[32m1\u001b[39m:] - \u001b[32m0.97\u001b[39m * y[:-\u001b[32m1\u001b[39m])\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 2) 멜 스펙트로그램 (파워 스펙트럼에 멜 필터뱅크 적용)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py:176\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    174\u001b[39m     \u001b[38;5;66;03m# Otherwise try soundfile first, and then fall back if necessary\u001b[39;00m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m         y, sr_native = \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m sf.SoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    179\u001b[39m         \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n\u001b[32m    180\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib.PurePath)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py:209\u001b[39m, in \u001b[36m__soundfile_load\u001b[39m\u001b[34m(path, offset, duration, dtype)\u001b[39m\n\u001b[32m    206\u001b[39m     context = path\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     context = \u001b[43msf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n\u001b[32m    212\u001b[39m     sr_native = sf_desc.samplerate\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\soundfile.py:690\u001b[39m, in \u001b[36mSoundFile.__init__\u001b[39m\u001b[34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\u001b[39m\n\u001b[32m    687\u001b[39m \u001b[38;5;28mself\u001b[39m._bitrate_mode = bitrate_mode\n\u001b[32m    688\u001b[39m \u001b[38;5;28mself\u001b[39m._info = _create_info_struct(file, mode, samplerate, channels,\n\u001b[32m    689\u001b[39m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m \u001b[38;5;28mself\u001b[39m._file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode).issuperset(\u001b[33m'\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.seekable():\n\u001b[32m    692\u001b[39m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[32m    693\u001b[39m     \u001b[38;5;28mself\u001b[39m.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\soundfile.py:1254\u001b[39m, in \u001b[36mSoundFile._open\u001b[39m\u001b[34m(self, file, mode_int, closefd)\u001b[39m\n\u001b[32m   1252\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m             file = file.encode(_sys.getfilesystemencoding())\n\u001b[32m-> \u001b[39m\u001b[32m1254\u001b[39m     file_ptr = \u001b[43mopenfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m   1256\u001b[39m     file_ptr = _snd.sf_open_fd(file, mode_int, \u001b[38;5;28mself\u001b[39m._info, closefd)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "mfcc_root = \"/data/mfcc_features\"\n",
    "os.makedirs(mfcc_root, exist_ok=True)\n",
    "\n",
    "# 3) 전처리 루프\n",
    "for wav_path in tqdm(wav_list, desc=\"Extracting MFCC\"):\n",
    "    try:\n",
    "        mfcc = extract_mfcc(\n",
    "            audio_path=wav_path,\n",
    "            sr=16000,\n",
    "            n_fft=512,\n",
    "            hop_length=160,\n",
    "            n_mfcc=20\n",
    "        )  # returns shape (frames, n_mfcc*3)\n",
    "        \n",
    "        # 파일명 기준으로 저장\n",
    "        fname = os.path.splitext(os.path.basename(wav_path))[0]\n",
    "        out_path = os.path.join(mfcc_root, f\"{fname}_mfcc.npy\")\n",
    "        np.save(out_path, mfcc)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # 오류가 난 파일은 로그로 남겨두기\n",
    "        with open(\"mfcc_errors.log\", \"a\") as log:\n",
    "            log.write(f\"{wav_path}\\t{e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cc1db52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8644aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    labels = eval_pred.label_ids\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3288872b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SSAFY\\AppData\\Local\\Temp\\ipykernel_33828\\1326427802.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9000/9000 30:03, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.958300</td>\n",
       "      <td>1.953369</td>\n",
       "      <td>0.104444</td>\n",
       "      <td>0.010909</td>\n",
       "      <td>0.104444</td>\n",
       "      <td>0.019754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.919800</td>\n",
       "      <td>1.964084</td>\n",
       "      <td>0.104444</td>\n",
       "      <td>0.010909</td>\n",
       "      <td>0.104444</td>\n",
       "      <td>0.019754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.943400</td>\n",
       "      <td>1.949484</td>\n",
       "      <td>0.104444</td>\n",
       "      <td>0.010909</td>\n",
       "      <td>0.104444</td>\n",
       "      <td>0.019754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.933200</td>\n",
       "      <td>1.950270</td>\n",
       "      <td>0.104444</td>\n",
       "      <td>0.010909</td>\n",
       "      <td>0.104444</td>\n",
       "      <td>0.019754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.955400</td>\n",
       "      <td>1.953544</td>\n",
       "      <td>0.104444</td>\n",
       "      <td>0.010909</td>\n",
       "      <td>0.104444</td>\n",
       "      <td>0.019754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9000, training_loss=1.9493119689093696, metrics={'train_runtime': 1804.2595, 'train_samples_per_second': 4.988, 'train_steps_per_second': 4.988, 'total_flos': 3.013354331487861e+17, 'train_loss': 1.9493119689093696, 'epoch': 5.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import librosa\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "train_dataset = EmotionDataset(train_files, train_labels)\n",
    "test_dataset = EmotionDataset(test_files, test_labels)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,  # VRAM 적으면 1로\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    fp16=False,  # 3050은 mixed precision 지원\n",
    "    learning_rate=1e-4,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=processor,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38a389ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGzCAYAAAAFROyYAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJwVJREFUeJzt3QlcVXX+//EPi+AKhAZIImnue+MWqWVKkprpZJM5plaOPnLQUsscGse1pHEqNQe1xdRmMrUmNRlTEbdxwo0elkuZmJOaCpUJaiMq3P/j8/3NvX8ugoGB9wu8no/H8XLO+d5zv+eA3Dff5Vwvh8PhEAAAAIt4e7oCAAAA+RFQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFCAMu4///mPeHl5ycsvv1xix9yyZYs5pj6WtClTpphj3whdu3Y1S/7z+uCDD27I6z/22GNy66233pDXAsobAgrgAYsXLzZvlHv27JHycB7OpXLlyhIeHi4xMTHy2muvyblz50rkdU6ePGmCzd69e8U2NtcNKMsIKAB+sWnTpsnf/vY3mT9/vowePdpsGzNmjLRs2VI+//xzt7ITJ06U//73v8UOAVOnTi12CNiwYYNZStO16vbmm2/KoUOHSvX1gfLK19MVAFD29ezZU9q1a+daj4uLk02bNsn9998vDzzwgHzxxRdSpUoVs8/X19cspemnn36SqlWrip+fn3hSpUqVPPr6QFlGCwpgqUuXLsmkSZOkbdu2EhgYKNWqVZMuXbrI5s2bC33OrFmzJDIy0oSBu+++W/bv339VmS+//FIeeughCQ4ONl0yGiw++uijEq9/t27d5E9/+pN888038ve///2aY1CSkpKkc+fOEhQUJNWrV5fGjRvL888/7xo30r59e/P1448/7upO0u4lpWNMWrRoIampqXLXXXeZYOJ8bv4xKE45OTmmTFhYmLmuGqKOHz/uVkbHjugYkvzyHvPn6lbQGJQLFy7IM888IxEREeLv72/OVccP5f9geT3OqFGjZNWqVeb8tGzz5s1l3bp1xfguAGUXLSiApbKysuStt96SgQMHyvDhw814joULF5rxHbt27ZI2bdq4lX/nnXdMmdjYWLl48aLMmTPHhIR9+/ZJaGioKXPgwAHp1KmT3HLLLfKHP/zBvDmvWLFC+vXrJ//4xz/k17/+dYmew+DBg00Q0G4WPYeCaJ20paVVq1amq0jfiNPS0uTf//632d+0aVOzXcPaiBEjTEhTd955p+sYP/zwg2nFeeSRR+TRRx91nW9hXnzxRRMAJkyYIBkZGTJ79myJjo423TTOlp6iKErd8tIQomFIQ+awYcPM93D9+vUyfvx4+fbbb03AzGv79u3y4Ycfyu9//3upUaOGGdfTv39/OXbsmNSsWbPI9QTKJAeAG27RokX657Jj9+7dhZa5cuWKIzs7223bjz/+6AgNDXU88cQTrm1Hjx41x6pSpYrjxIkTru07d+4028eOHeva1r17d0fLli0dFy9edG3Lzc113HnnnY6GDRu6tm3evNk8Vx9/6XkEBgY6br/9dtf65MmTzXOcZs2aZda/++67Qo+hx9cy+nr53X333WbfggULCtynS/7zuuWWWxxZWVmu7StWrDDb58yZ49oWGRnpGDp06M8e81p10+frcZxWrVplyr7wwgtu5R566CGHl5eXIy0tzbVNy/n5+blt++yzz8z2uXPnFnKlgPKDLh7AUj4+Pq4xFLm5uXLmzBm5cuWK6ZL59NNPryqvrSDaMuLUoUMH6dixo6xdu9as6/N1XMjDDz9sWlq+//57s2jrg7bKHD582PwVX9K0y+Zas3m0W0etXr3anOf10FYX7WIpqiFDhpgWCSft8qpdu7brWpUWPb5+X5966im37drlo5nk448/dtuurTq33Xaba11bmQICAuTrr78u1XoCNiCgABZbsmSJeVPSsSLapH/zzTfLP//5T8nMzLyqbMOGDa/a1qhRI3OfFKXdJvomqONC9Dh5l8mTJ5sy2t1R0s6fP+8WBvIbMGCA6Xb63e9+Z7pmtJtGu52KE1Y0mBVnQGz+a6XdPQ0aNHBdq9Ki43F0Gnb+66FdRc79edWtW/eqY9x0003y448/lmo9ARswBgWwlA4s1UGW2jKiYxRCQkLMX9/x8fFy5MiRYh/P+Yb/7LPPmhaTguibdEk6ceKECVPXOq6O+di2bZsZl6HhSweBLl++3Iyf0bEres4/pzjjRoqqsJvJ6QDbotSpJBT2OvkH1ALlEQEFsJTe7bR+/fpmkGTeN0tna0d+2kWT31dffeWaRaLHck591a6DG0HvjaIKC0RO3t7e0r17d7O8+uqrMmPGDPnjH/9oQovWtaTvPJv/WukbvrYwaWtV3paKs2fPXvVcbeVwXktVnLrpDKuNGzeaLq+8rSg6s8q5H8D/oYsHsJTzr+e8fy3v3LlTUlJSCiyv01HzjiHRmT5aXme3KG2B0emxr7/+upw6deqq53/33XclWn8d7zJ9+nSpV6+eDBo0qNByOjYmP+cMpezsbPOos41UQYHhejhnPOUNg3pNnNdK6diPHTt2mOneTomJiVdNRy5O3Xr16mVaYP7617+6bdfZOxp08r4+UNHRggJ40Ntvv13gfS2efvppM/VWW0906m/v3r3l6NGjsmDBAmnWrJkZ15GfdqPovURGjhxp3th16qyOW3nuuedcZRISEkwZvcOrTvvVloD09HQTerQ75rPPPruu89DBndoKoIN49XgaTvTeJtoioPdY0TE0hdFputrFo+eo5XUczLx586ROnTqmrs6woINp9fy15UFDgQ4A1vBzPfQeMHpsHVir9dVrpdcv71RoHROjweW+++4zA4u1W0273fIOWi1u3fr06SP33HOPaR3S8S6tW7c23Vg6QFjvvJv/2ECF5ulpREBF5JyeW9hy/PhxM/13xowZZpqqv7+/maqbmJh41dRV5zTjv/zlL45XXnnFERERYcp36dLFTEvN78iRI44hQ4Y4wsLCHJUqVTJTbu+//37HBx98cN3TjJ2LTovV4957771mym7eqbyFTTNOTk529O3b1xEeHm6er48DBw50fPXVV27PW716taNZs2YOX19ft2m9OuW3efPmBdavsGnG7733niMuLs4REhJipmf37t3b8c0331z1fL2een30enbq1MmxZ8+eq455rbrl/16pc+fOmanfep56/XV6t37v9Pudlx4nNjb2qjoVNv0ZKG+89B9PhyQAAIC8GIMCAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGCdMnmjNv1MkZMnT5qbIpX0LbABAEDp0Dub6F2c9UMz9SMuyl1A0XASERHh6WoAAIDroB8ZoXeLLncBxfkhW3qCAQEBnq4OAAAogqysLNPAkPfDMstVQHF262g4IaAAAFC2FGV4BoNkAQCAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKzj6+kK2OjWP/xTypr/vNTb01UAAKDE0IICAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANbhVvflBLfnvzG4zjdGWbzOZRE/G7D554MWFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAKBsB5T58+dLq1atJCAgwCxRUVHy8ccfu/ZfvHhRYmNjpWbNmlK9enXp37+/pKenux3j2LFj0rt3b6lataqEhITI+PHj5cqVKyV3RgAAoGIFlDp16shLL70kqampsmfPHunWrZv07dtXDhw4YPaPHTtW1qxZI++//75s3bpVTp48KQ8++KDr+Tk5OSacXLp0ST755BNZsmSJLF68WCZNmlTyZwYAAMos3+IU7tOnj9v6iy++aFpVduzYYcLLwoULZenSpSa4qEWLFknTpk3N/jvuuEM2bNggBw8elI0bN0poaKi0adNGpk+fLhMmTJApU6aIn59fyZ4dAACoWGNQtDVk2bJlcuHCBdPVo60qly9flujoaFeZJk2aSN26dSUlJcWs62PLli1NOHGKiYmRrKwsVytMQbKzs02ZvAsAACi/ih1Q9u3bZ8aX+Pv7y5NPPikrV66UZs2ayenTp00LSFBQkFt5DSO6T+lj3nDi3O/cV5j4+HgJDAx0LREREcWtNgAAKM8BpXHjxrJ3717ZuXOnjBw5UoYOHWq6bUpTXFycZGZmupbjx4+X6usBAIAyNAZFaStJgwYNzNdt27aV3bt3y5w5c2TAgAFm8OvZs2fdWlF0Fk9YWJj5Wh937drldjznLB9nmYJoa40uAACgYvjF90HJzc01Y0Q0rFSqVEmSk5Nd+w4dOmSmFesYFaWP2kWUkZHhKpOUlGSmLGs3EQAAQLFbULSrpWfPnmbg67lz58yMnS1btsj69evN2JBhw4bJuHHjJDg42ISO0aNHm1CiM3hUjx49TBAZPHiwzJw504w7mThxorl3Ci0kAADgugKKtnwMGTJETp06ZQKJ3rRNw8m9995r9s+aNUu8vb3NDdq0VUVn6MybN8/1fB8fH0lMTDRjVzS4VKtWzYxhmTZtWnGqAQAAyrliBRS9z8m1VK5cWRISEsxSmMjISFm7dm1xXhYAAFQwfBYPAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAAynZAiY+Pl/bt20uNGjUkJCRE+vXrJ4cOHXIr07VrV/Hy8nJbnnzySbcyx44dk969e0vVqlXNccaPHy9XrlwpmTMCAABlnm9xCm/dulViY2NNSNFA8fzzz0uPHj3k4MGDUq1aNVe54cOHy7Rp01zrGkSccnJyTDgJCwuTTz75RE6dOiVDhgyRSpUqyYwZM0rqvAAAQEUJKOvWrXNbX7x4sWkBSU1NlbvuusstkGgAKciGDRtMoNm4caOEhoZKmzZtZPr06TJhwgSZMmWK+Pn5Xe+5AACAcuIXjUHJzMw0j8HBwW7b3333XalVq5a0aNFC4uLi5KeffnLtS0lJkZYtW5pw4hQTEyNZWVly4MCBAl8nOzvb7M+7AACA8qtYLSh55ebmypgxY6RTp04miDj99re/lcjISAkPD5fPP//ctIzoOJUPP/zQ7D99+rRbOFHOdd1X2NiXqVOnXm9VAQBARQkoOhZl//79sn37drftI0aMcH2tLSW1a9eW7t27y5EjR+S22267rtfSVphx48a51rUFJSIi4nqrDgAAymMXz6hRoyQxMVE2b94sderUuWbZjh07mse0tDTzqGNT0tPT3co41wsbt+Lv7y8BAQFuCwAAKL+KFVAcDocJJytXrpRNmzZJvXr1fvY5e/fuNY/akqKioqJk3759kpGR4SqTlJRkQkezZs2KfwYAAKBid/Fot87SpUtl9erV5l4ozjEjgYGBUqVKFdONo/t79eolNWvWNGNQxo4da2b4tGrVypTVackaRAYPHiwzZ840x5g4caI5traUAAAAFKsFZf78+Wbmjt6MTVtEnMvy5cvNfp0irNOHNYQ0adJEnnnmGenfv7+sWbPGdQwfHx/TPaSP2pry6KOPmvug5L1vCgAAqNh8i9vFcy06cFVv5vZzdJbP2rVri/PSAACgAuGzeAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAKBsB5T4+Hhp37691KhRQ0JCQqRfv35y6NAhtzIXL16U2NhYqVmzplSvXl369+8v6enpbmWOHTsmvXv3lqpVq5rjjB8/Xq5cuVIyZwQAACpWQNm6dasJHzt27JCkpCS5fPmy9OjRQy5cuOAqM3bsWFmzZo28//77pvzJkyflwQcfdO3Pyckx4eTSpUvyySefyJIlS2Tx4sUyadKkkj0zAABQZvkWp/C6devc1jVYaAtIamqq3HXXXZKZmSkLFy6UpUuXSrdu3UyZRYsWSdOmTU2oueOOO2TDhg1y8OBB2bhxo4SGhkqbNm1k+vTpMmHCBJkyZYr4+fmV7BkCAICKNQZFA4kKDg42jxpUtFUlOjraVaZJkyZSt25dSUlJMev62LJlSxNOnGJiYiQrK0sOHDhQ4OtkZ2eb/XkXAABQfl13QMnNzZUxY8ZIp06dpEWLFmbb6dOnTQtIUFCQW1kNI7rPWSZvOHHud+4rbOxLYGCga4mIiLjeagMAgPIcUHQsyv79+2XZsmVS2uLi4kxrjXM5fvx4qb8mAAAoI2NQnEaNGiWJiYmybds2qVOnjmt7WFiYGfx69uxZt1YUncWj+5xldu3a5XY85ywfZ5n8/P39zQIAACqGYrWgOBwOE05WrlwpmzZtknr16rntb9u2rVSqVEmSk5Nd23Qask4rjoqKMuv6uG/fPsnIyHCV0RlBAQEB0qxZs19+RgAAoGK1oGi3js7QWb16tbkXinPMiI4LqVKlinkcNmyYjBs3zgyc1dAxevRoE0p0Bo/SackaRAYPHiwzZ840x5g4caI5Nq0kAACg2AFl/vz55rFr165u23Uq8WOPPWa+njVrlnh7e5sbtOnsG52hM2/ePFdZHx8f0z00cuRIE1yqVasmQ4cOlWnTpvEdAQAAxQ8o2sXzcypXriwJCQlmKUxkZKSsXbu2OC8NAAAqED6LBwAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAMp+QNm2bZv06dNHwsPDxcvLS1atWuW2/7HHHjPb8y733XefW5kzZ87IoEGDJCAgQIKCgmTYsGFy/vz5X342AACgYgaUCxcuSOvWrSUhIaHQMhpITp065Vree+89t/0aTg4cOCBJSUmSmJhoQs+IESOu7wwAAEC541vcJ/Ts2dMs1+Lv7y9hYWEF7vviiy9k3bp1snv3bmnXrp3ZNnfuXOnVq5e8/PLLpmUGAABUbKUyBmXLli0SEhIijRs3lpEjR8oPP/zg2peSkmK6dZzhREVHR4u3t7fs3LmzwONlZ2dLVlaW2wIAAMqvEg8o2r3zzjvvSHJysvz5z3+WrVu3mhaXnJwcs//06dMmvOTl6+srwcHBZl9B4uPjJTAw0LVERESUdLUBAEBZ7uL5OY888ojr65YtW0qrVq3ktttuM60q3bt3v65jxsXFybhx41zr2oJCSAEAoPwq9WnG9evXl1q1aklaWppZ17EpGRkZbmWuXLliZvYUNm5Fx7TojJ+8CwAAKL9KPaCcOHHCjEGpXbu2WY+KipKzZ89Kamqqq8ymTZskNzdXOnbsWNrVAQAA5bGLR+9X4mwNUUePHpW9e/eaMSS6TJ06Vfr3729aQ44cOSLPPfecNGjQQGJiYkz5pk2bmnEqw4cPlwULFsjly5dl1KhRpmuIGTwAAOC6WlD27Nkjt99+u1mUjg3RrydNmiQ+Pj7y+eefywMPPCCNGjUyN2Br27at/Otf/zLdNE7vvvuuNGnSxIxJ0enFnTt3ljfeeIPvCAAAuL4WlK5du4rD4Sh0//r163/2GNrSsnTp0uK+NAAAqCD4LB4AAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAACU/YCybds26dOnj4SHh4uXl5esWrXKbb/D4ZBJkyZJ7dq1pUqVKhIdHS2HDx92K3PmzBkZNGiQBAQESFBQkAwbNkzOnz//y88GAABUzIBy4cIFad26tSQkJBS4f+bMmfLaa6/JggULZOfOnVKtWjWJiYmRixcvuspoODlw4IAkJSVJYmKiCT0jRoz4ZWcCAADKDd/iPqFnz55mKYi2nsyePVsmTpwoffv2NdveeecdCQ0NNS0tjzzyiHzxxReybt062b17t7Rr186UmTt3rvTq1Utefvll0zIDAAAqthIdg3L06FE5ffq06dZxCgwMlI4dO0pKSopZ10ft1nGGE6Xlvb29TYtLQbKzsyUrK8ttAQAA5VeJBhQNJ0pbTPLSdec+fQwJCXHb7+vrK8HBwa4y+cXHx5ug41wiIiJKstoAAMAyZWIWT1xcnGRmZrqW48ePe7pKAACgrASUsLAw85ienu62Xded+/QxIyPDbf+VK1fMzB5nmfz8/f3NjJ+8CwAAKL9KNKDUq1fPhIzk5GTXNh0vomNLoqKizLo+nj17VlJTU11lNm3aJLm5uWasCgAAQLFn8ej9StLS0twGxu7du9eMIalbt66MGTNGXnjhBWnYsKEJLH/605/MzJx+/fqZ8k2bNpX77rtPhg8fbqYiX758WUaNGmVm+DCDBwAAXFdA2bNnj9xzzz2u9XHjxpnHoUOHyuLFi+W5554z90rR+5poS0nnzp3NtOLKlSu7nvPuu++aUNK9e3cze6d///7m3ikAAADXFVC6du1q7ndSGL277LRp08xSGG1tWbp0Kd8BAABQdmfxAACAioWAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAMp/QJkyZYp4eXm5LU2aNHHtv3jxosTGxkrNmjWlevXq0r9/f0lPTy/pagAAgDKsVFpQmjdvLqdOnXIt27dvd+0bO3asrFmzRt5//33ZunWrnDx5Uh588MHSqAYAACijfEvloL6+EhYWdtX2zMxMWbhwoSxdulS6detmti1atEiaNm0qO3bskDvuuKM0qgMAAMqYUmlBOXz4sISHh0v9+vVl0KBBcuzYMbM9NTVVLl++LNHR0a6y2v1Tt25dSUlJKfR42dnZkpWV5bYAAIDyq8QDSseOHWXx4sWybt06mT9/vhw9elS6dOki586dk9OnT4ufn58EBQW5PSc0NNTsK0x8fLwEBga6loiIiJKuNgAAKM9dPD179nR93apVKxNYIiMjZcWKFVKlSpXrOmZcXJyMGzfOta4tKIQUAADKr1KfZqytJY0aNZK0tDQzLuXSpUty9uxZtzI6i6egMStO/v7+EhAQ4LYAAIDyq9QDyvnz5+XIkSNSu3Ztadu2rVSqVEmSk5Nd+w8dOmTGqERFRZV2VQAAQEXt4nn22WelT58+pltHpxBPnjxZfHx8ZODAgWb8yLBhw0x3TXBwsGkJGT16tAknzOABAAClFlBOnDhhwsgPP/wgN998s3Tu3NlMIdav1axZs8Tb29vcoE1n58TExMi8efNKuhoAAKAMK/GAsmzZsmvur1y5siQkJJgFAACgIHwWDwAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFjHowElISFBbr31VqlcubJ07NhRdu3a5cnqAACAih5Qli9fLuPGjZPJkyfLp59+Kq1bt5aYmBjJyMjwVJUAAEBFDyivvvqqDB8+XB5//HFp1qyZLFiwQKpWrSpvv/22p6oEAAAs4euJF7106ZKkpqZKXFyca5u3t7dER0dLSkrKVeWzs7PN4pSZmWkes7KySqV+udk/lcpx4a60vn+lqSz+bHCdURh+NnCjfz6cx3Q4HHYGlO+//15ycnIkNDTUbbuuf/nll1eVj4+Pl6lTp161PSIiolTridIVONvTNagYuM4oDD8b8NTPx7lz5yQwMNC+gFJc2tKi41WccnNz5cyZM1KzZk3x8vIq8XSnwef48eMSEBBQoscub7hWRce1KjquVdFxrYqOa2XH9dKWEw0n4eHhP1vWIwGlVq1a4uPjI+np6W7bdT0sLOyq8v7+/mbJKygoqFTrqN8QfoiLhmtVdFyrouNaFR3Xqui4Vp6/Xj/XcuLRQbJ+fn7Stm1bSU5OdmsV0fWoqChPVAkAAFjEY1082mUzdOhQadeunXTo0EFmz54tFy5cMLN6AABAxeaxgDJgwAD57rvvZNKkSXL69Glp06aNrFu37qqBszeadiXpvVnydynhalyrouNaFR3Xqui4VkXHtSp718vLUZS5PgAAADcQn8UDAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BJQ8EhIS5NZbb5XKlStLx44dZdeuXZ6ukpW2bdsmffr0Mbcq1o8aWLVqlaerZC39HKn27dtLjRo1JCQkRPr16yeHDh3ydLWsNH/+fGnVqpXrzpV608aPP/7Y09UqE1566SXzf3HMmDGerop1pkyZYq5N3qVJkyaerpa1vv32W3n00UfNR8lUqVJFWrZsKXv27PFIXQgo/7N8+XJz8zid9/3pp59K69atJSYmRjIyMjxdNevoDfX0+migw7Vt3bpVYmNjZceOHZKUlCSXL1+WHj16mGsId3Xq1DFvtPpJ5/oLsVu3btK3b185cOCAp6tmtd27d8vrr79uwh0K1rx5czl16pRr2b59u6erZKUff/xROnXqJJUqVTJ/HBw8eFBeeeUVuemmmzxTIb0PChyODh06OGJjY13rOTk5jvDwcEd8fLxH62U7/RFauXKlp6tRZmRkZJhrtnXrVk9XpUy46aabHG+99Zanq2Gtc+fOORo2bOhISkpy3H333Y6nn37a01WyzuTJkx2tW7f2dDXKhAkTJjg6d+7ssAUtKCJy6dIl81dbdHS0a5u3t7dZT0lJ8WjdUL5kZmaax+DgYE9XxWo5OTmybNky09LE53MVTlvnevfu7fa7C1c7fPiw6ZKuX7++DBo0SI4dO+bpKlnpo48+Mh8/85vf/MZ0Sd9+++3y5ptveqw+BBQR+f77780vxPy32dd1vQ0/UBL0AzF1jIA2obZo0cLT1bHSvn37pHr16ub22k8++aSsXLlSmjVr5ulqWUkDnHZH6zgnFE7HEy5evNh8lIqOczp69Kh06dJFzp075+mqWefrr78216hhw4ayfv16GTlypDz11FOyZMmSivVZPEBF/Gt3//799H9fQ+PGjWXv3r2mpemDDz4wHyiq43gIKe6OHz8uTz/9tBnXpIP6UbiePXu6vtZxOhpYIiMjZcWKFTJs2DCP1s3GP6LatWsnM2bMMOvagqK/sxYsWGD+L95otKCISK1atcTHx0fS09Pdtut6WFiYx+qF8mPUqFGSmJgomzdvNoNBUTA/Pz9p0KCBtG3b1rQM6GDsOXPmeLpa1tEuaR3A/6tf/Up8fX3NokHutddeM19rizAKFhQUJI0aNZK0tDRPV8U6tWvXvuqPgaZNm3qsS4yA8r9fivoLMTk52S1J6jr93/gldByxhhPtqti0aZPUq1fP01UqU/T/YXZ2tqerYZ3u3bub7jBtbXIu+pevjq/Qr/UPLhTs/PnzcuTIEfNmDHfa/Zz/NghfffWVaXHyBLp4/kenGGsTlv4n79Chg8yePdsM0Hv88cc9XTUr/4Pn/etD+3T1l6IO/Kxbt65H62Zjt87SpUtl9erV5l4ozjFNgYGB5h4D+P/i4uJMc7z+DOn4AL1uW7ZsMX3hcKc/S/nHMVWrVs3cu4LxTe6effZZc98mfZM9efKkuZWEBriBAwd6umrWGTt2rNx5552mi+fhhx829wJ74403zOIRnp5GZJO5c+c66tat6/Dz8zPTjnfs2OHpKllp8+bNZqps/mXo0KGerpp1CrpOuixatMjTVbPOE0884YiMjDT//26++WZH9+7dHRs2bPB0tcoMphkXbMCAAY7atWubn6tbbrnFrKelpXm6WtZas2aNo0WLFg5/f39HkyZNHG+88YbH6uKl/3gmGgEAABSMMSgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAENv8PyRuOrhjZHbMAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(label_list)\n",
    "plt.title(\"Label Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d9062b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n",
      "450\n",
      "2250\n",
      "2250\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(label_list))\n",
    "print(len(wav_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efdd2457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[ 0.00122942, -0.05914168,  0.07904113, ..., -0.07488912,\n",
      "         0.00148852, -0.03032699],\n",
      "       [ 0.00122942, -0.0591417 ,  0.07904113, ..., -0.07488912,\n",
      "         0.00148852, -0.03032699],\n",
      "       [ 0.00122942, -0.05914169,  0.07904113, ..., -0.07488912,\n",
      "         0.00148851, -0.03032699],\n",
      "       ...,\n",
      "       [ 0.00122942, -0.05914169,  0.07904113, ..., -0.07488911,\n",
      "         0.00148852, -0.03032699],\n",
      "       [ 0.00122942, -0.0591417 ,  0.07904113, ..., -0.07488912,\n",
      "         0.00148852, -0.03032699],\n",
      "       [ 0.00122942, -0.05914169,  0.07904114, ..., -0.07488912,\n",
      "         0.00148852, -0.03032699]], shape=(450, 7), dtype=float32), label_ids=array([2, 1, 1, 0, 5, 5, 5, 1, 5, 4, 0, 4, 4, 6, 3, 4, 2, 6, 3, 4, 0, 1,\n",
      "       3, 4, 6, 5, 2, 1, 6, 0, 4, 2, 2, 2, 3, 2, 6, 5, 2, 5, 0, 6, 3, 0,\n",
      "       6, 6, 4, 4, 1, 5, 4, 0, 6, 6, 6, 4, 6, 2, 0, 6, 6, 0, 4, 1, 6, 5,\n",
      "       5, 2, 1, 0, 4, 5, 5, 1, 3, 2, 4, 4, 0, 2, 1, 5, 5, 5, 4, 5, 0, 4,\n",
      "       0, 3, 6, 5, 4, 5, 4, 0, 1, 6, 0, 4, 6, 0, 4, 2, 2, 0, 3, 1, 2, 3,\n",
      "       6, 0, 6, 4, 6, 1, 1, 3, 5, 6, 6, 3, 4, 5, 2, 5, 4, 4, 2, 0, 1, 2,\n",
      "       2, 6, 3, 0, 4, 4, 3, 3, 6, 5, 5, 0, 6, 5, 2, 5, 2, 6, 1, 0, 0, 1,\n",
      "       1, 4, 1, 0, 6, 1, 0, 3, 1, 3, 6, 1, 0, 2, 1, 6, 6, 4, 4, 4, 2, 1,\n",
      "       0, 0, 3, 6, 5, 3, 0, 1, 4, 4, 3, 6, 6, 0, 3, 2, 3, 4, 6, 4, 1, 3,\n",
      "       4, 4, 3, 3, 2, 4, 4, 4, 5, 2, 2, 4, 5, 5, 3, 0, 0, 1, 5, 1, 6, 1,\n",
      "       0, 4, 1, 5, 0, 5, 4, 1, 3, 1, 3, 6, 4, 0, 3, 3, 4, 6, 0, 0, 5, 0,\n",
      "       0, 2, 3, 6, 6, 5, 3, 3, 1, 0, 5, 4, 1, 6, 0, 1, 4, 3, 2, 3, 6, 5,\n",
      "       1, 1, 3, 1, 4, 6, 2, 1, 1, 1, 6, 1, 3, 3, 3, 2, 6, 3, 6, 0, 6, 0,\n",
      "       6, 1, 3, 2, 4, 0, 1, 1, 3, 3, 1, 5, 3, 6, 3, 6, 6, 0, 2, 6, 1, 5,\n",
      "       5, 6, 1, 4, 1, 0, 5, 1, 1, 1, 1, 1, 1, 0, 6, 0, 0, 4, 0, 5, 4, 1,\n",
      "       3, 4, 1, 4, 0, 5, 0, 3, 5, 6, 5, 0, 1, 5, 2, 4, 6, 3, 0, 2, 3, 0,\n",
      "       2, 2, 0, 2, 0, 3, 6, 4, 2, 6, 6, 5, 5, 1, 4, 6, 1, 2, 1, 1, 5, 0,\n",
      "       2, 0, 0, 6, 4, 3, 3, 0, 5, 6, 0, 2, 4, 1, 5, 1, 2, 1, 5, 5, 0, 1,\n",
      "       2, 0, 3, 4, 1, 1, 5, 1, 1, 5, 5, 6, 1, 2, 4, 4, 3, 1, 4, 0, 6, 5,\n",
      "       0, 5, 3, 4, 1, 4, 5, 5, 1, 4, 6, 0, 4, 5, 1, 1, 0, 3, 0, 0, 3, 2,\n",
      "       4, 6, 4, 2, 5, 0, 0, 1, 3, 6]), metrics={'test_loss': 1.9535444974899292, 'test_accuracy': 0.10444444444444445, 'test_precision': 0.010908641975308641, 'test_recall': 0.10444444444444445, 'test_f1': 0.019754080035770175, 'test_runtime': 12.5295, 'test_samples_per_second': 35.915, 'test_steps_per_second': 35.915})\n",
      "[[ 0.00122942 -0.05914168  0.07904113 ... -0.07488912  0.00148852\n",
      "  -0.03032699]\n",
      " [ 0.00122942 -0.0591417   0.07904113 ... -0.07488912  0.00148852\n",
      "  -0.03032699]\n",
      " [ 0.00122942 -0.05914169  0.07904113 ... -0.07488912  0.00148851\n",
      "  -0.03032699]\n",
      " ...\n",
      " [ 0.00122942 -0.05914169  0.07904113 ... -0.07488911  0.00148852\n",
      "  -0.03032699]\n",
      " [ 0.00122942 -0.0591417   0.07904113 ... -0.07488912  0.00148852\n",
      "  -0.03032699]\n",
      " [ 0.00122942 -0.05914169  0.07904114 ... -0.07488912  0.00148852\n",
      "  -0.03032699]]\n",
      "[2 1 1 0 5 5 5 1 5 4 0 4 4 6 3 4 2 6 3 4 0 1 3 4 6 5 2 1 6 0 4 2 2 2 3 2 6\n",
      " 5 2 5 0 6 3 0 6 6 4 4 1 5 4 0 6 6 6 4 6 2 0 6 6 0 4 1 6 5 5 2 1 0 4 5 5 1\n",
      " 3 2 4 4 0 2 1 5 5 5 4 5 0 4 0 3 6 5 4 5 4 0 1 6 0 4 6 0 4 2 2 0 3 1 2 3 6\n",
      " 0 6 4 6 1 1 3 5 6 6 3 4 5 2 5 4 4 2 0 1 2 2 6 3 0 4 4 3 3 6 5 5 0 6 5 2 5\n",
      " 2 6 1 0 0 1 1 4 1 0 6 1 0 3 1 3 6 1 0 2 1 6 6 4 4 4 2 1 0 0 3 6 5 3 0 1 4\n",
      " 4 3 6 6 0 3 2 3 4 6 4 1 3 4 4 3 3 2 4 4 4 5 2 2 4 5 5 3 0 0 1 5 1 6 1 0 4\n",
      " 1 5 0 5 4 1 3 1 3 6 4 0 3 3 4 6 0 0 5 0 0 2 3 6 6 5 3 3 1 0 5 4 1 6 0 1 4\n",
      " 3 2 3 6 5 1 1 3 1 4 6 2 1 1 1 6 1 3 3 3 2 6 3 6 0 6 0 6 1 3 2 4 0 1 1 3 3\n",
      " 1 5 3 6 3 6 6 0 2 6 1 5 5 6 1 4 1 0 5 1 1 1 1 1 1 0 6 0 0 4 0 5 4 1 3 4 1\n",
      " 4 0 5 0 3 5 6 5 0 1 5 2 4 6 3 0 2 3 0 2 2 0 2 0 3 6 4 2 6 6 5 5 1 4 6 1 2\n",
      " 1 1 5 0 2 0 0 6 4 3 3 0 5 6 0 2 4 1 5 1 2 1 5 5 0 1 2 0 3 4 1 1 5 1 1 5 5\n",
      " 6 1 2 4 4 3 1 4 0 6 5 0 5 3 4 1 4 5 5 1 4 6 0 4 5 1 1 0 3 0 0 3 2 4 6 4 2\n",
      " 5 0 0 1 3 6]\n"
     ]
    }
   ],
   "source": [
    "# test_dataset은 이미 생성되어 있다고 가정\n",
    "output = trainer.predict(test_dataset)\n",
    "logits = output.predictions\n",
    "labels = output.label_ids\n",
    "print(output)\n",
    "print(logits)\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6152208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000        72\n",
      "           1     0.0000    0.0000    0.0000        77\n",
      "           2     0.1044    1.0000    0.1891        47\n",
      "           3     0.0000    0.0000    0.0000        57\n",
      "           4     0.0000    0.0000    0.0000        69\n",
      "           5     0.0000    0.0000    0.0000        61\n",
      "           6     0.0000    0.0000    0.0000        67\n",
      "\n",
      "    accuracy                         0.1044       450\n",
      "   macro avg     0.0149    0.1429    0.0270       450\n",
      "weighted avg     0.0109    0.1044    0.0198       450\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds = np.argmax(logits, axis=1)\n",
    "print(classification_report(labels, preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d03997a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m preds = np.argmax(\u001b[43mlogits\u001b[49m, axis=\u001b[32m1\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(classification_report(labels, preds, digits=\u001b[32m4\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'logits' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds = np.argmax(logits, axis=1)\n",
    "print(classification_report(labels, preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "4ae09b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.57142857142857%\n"
     ]
    }
   ],
   "source": [
    "val_list = find_wav_files(BASE_DIR, False, 1)\n",
    "ans = {\n",
    "    '기쁨': 'Happy',\n",
    "    '슬픔': 'Sad',\n",
    "    '분노': 'Angry',\n",
    "    '당황': 'Embarrassed',\n",
    "    '상처': 'Hurt',\n",
    "    '중립': 'Neutrality',\n",
    "    '불안': 'Anxious'\n",
    "}\n",
    "\n",
    "res = 0\n",
    "\n",
    "for wav in val_list:\n",
    "    emo = wav.split(\"1.감정\\\\\")[1].split(\"\\\\\")[0][2:]\n",
    "    inferred = infer(model, wav, device)\n",
    "    if inferred == ans[emo]: res += 1\n",
    "res = res / len(val_list) * 100\n",
    "print(f'{res}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ad5fb97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.538\n",
      "Macro F1: 0.531078086798181\n",
      "Macro Recall: 0.5342598295980111\n",
      "Macro Precision: 0.5319771190610262\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7423    0.7934    0.7670       639\n",
      "           1     0.6358    0.6916    0.6625       694\n",
      "           2     0.4507    0.5306    0.4874       637\n",
      "           3     0.3595    0.3895    0.3739       611\n",
      "           4     0.3737    0.2900    0.3265       607\n",
      "           5     0.4860    0.4356    0.4594       675\n",
      "           6     0.6760    0.6091    0.6408       637\n",
      "\n",
      "    accuracy                         0.5380      4500\n",
      "   macro avg     0.5320    0.5343    0.5311      4500\n",
      "weighted avg     0.5350    0.5380    0.5345      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, recall_score, precision_score, accuracy_score\n",
    "\n",
    "# ... (모델 학습 및 평가 루프는 동일)\n",
    "\n",
    "# 1. 예측값과 실제값 수집\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "# 2. 주요 지표 출력\n",
    "print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "print(\"Macro F1:\", f1_score(all_labels, all_preds, average='macro'))\n",
    "print(\"Macro Recall:\", recall_score(all_labels, all_preds, average='macro'))\n",
    "print(\"Macro Precision:\", precision_score(all_labels, all_preds, average='macro'))\n",
    "\n",
    "# 3. 전체 리포트 (클래스별 지표 포함)\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d81e833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"cnn_1d_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
