{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9e7fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find platform independent libraries <prefix>\n"
     ]
    }
   ],
   "source": [
    "!python -m venv venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "375b69f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (2.7.0+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (2.7.0+cu118)\n",
      "Requirement already satisfied: librosa in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: standard-aifc in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: standard-sunau in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: standard-chunk in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from standard-aifc->librosa) (3.13.0)\n",
      "Requirement already satisfied: audioop-lts in c:\\users\\ssafy\\desktop\\_fpjt\\.venv\\lib\\site-packages (from standard-aifc->librosa) (0.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find platform independent libraries <prefix>\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchaudio librosa numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "72aa14d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def find_wav_files(base_dir, is_train:bool, per_class=100):\n",
    "    sub_root = '1.Training' if is_train else '2.Validation'\n",
    "\n",
    "    wav_files = []\n",
    "    emotion_dir = os.path.join(\n",
    "        base_dir, \n",
    "        '01.데이터', \n",
    "        sub_root,\n",
    "        '원천데이터',\n",
    "        ('T' if is_train else 'V') + 'S1',\n",
    "        ('T' if is_train else 'V') + 'S1',\n",
    "        '1.감정'\n",
    "    )\n",
    "    \n",
    "    for emo in os.listdir(emotion_dir):\n",
    "        emo_path = os.path.join(emotion_dir, emo)\n",
    "        if not os.path.isdir(emo_path): continue\n",
    "        for sub_folder in os.listdir(emo_path):\n",
    "            sub_emo_path = os.path.join(emo_path, sub_folder)\n",
    "            if not os.path.isdir(sub_emo_path): continue\n",
    "\n",
    "            wav_list = [\n",
    "                os.path.join(sub_emo_path, f)\n",
    "                for f in os.listdir(sub_emo_path) if f.endswith('.wav')\n",
    "            ]\n",
    "            if len(wav_list) == 0:\n",
    "                continue\n",
    "            \n",
    "            sampled = random.sample(wav_list, min(per_class, len(wav_list)))\n",
    "            wav_files.extend(sampled)\n",
    "    return wav_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "601f744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = r'C:\\Users\\SSAFY\\Downloads\\015.감성 및 발화 스타일별 음성합성 데이터'\n",
    "wav_list = find_wav_files(BASE_DIR, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08a418f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\SSAFY\\\\Downloads\\\\015.감성 및 발화 스타일별 음성합성 데이터\\\\01.데이터\\\\1.Training\\\\원천데이터\\\\TS1\\\\TS1\\\\1.감정\\\\1.기쁨\\\\0001_G1A3E1S0C0_PSB\\\\0001_G1A3E1S0C0_PSB_001047.wav',\n",
       " 'C:\\\\Users\\\\SSAFY\\\\Downloads\\\\015.감성 및 발화 스타일별 음성합성 데이터\\\\01.데이터\\\\1.Training\\\\원천데이터\\\\TS1\\\\TS1\\\\1.감정\\\\1.기쁨\\\\0001_G1A3E1S0C0_PSB\\\\0001_G1A3E1S0C0_PSB_000283.wav',\n",
       " 'C:\\\\Users\\\\SSAFY\\\\Downloads\\\\015.감성 및 발화 스타일별 음성합성 데이터\\\\01.데이터\\\\1.Training\\\\원천데이터\\\\TS1\\\\TS1\\\\1.감정\\\\1.기쁨\\\\0001_G1A3E1S0C0_PSB\\\\0001_G1A3E1S0C0_PSB_001411.wav',\n",
       " 'C:\\\\Users\\\\SSAFY\\\\Downloads\\\\015.감성 및 발화 스타일별 음성합성 데이터\\\\01.데이터\\\\1.Training\\\\원천데이터\\\\TS1\\\\TS1\\\\1.감정\\\\1.기쁨\\\\0001_G1A3E1S0C0_PSB\\\\0001_G1A3E1S0C0_PSB_001911.wav',\n",
       " 'C:\\\\Users\\\\SSAFY\\\\Downloads\\\\015.감성 및 발화 스타일별 음성합성 데이터\\\\01.데이터\\\\1.Training\\\\원천데이터\\\\TS1\\\\TS1\\\\1.감정\\\\1.기쁨\\\\0001_G1A3E1S0C0_PSB\\\\0001_G1A3E1S0C0_PSB_001724.wav']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wav_list)\n",
    "wav_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ef5847c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\SSAFY\\\\Downloads\\\\015.감성 및 발화 스타일별 음성합성 데이터\\\\01.데이터\\\\1.Training\\\\라벨링데이터\\\\TL1\\\\TL1\\\\1.감정\\\\1.기쁨\\\\0001_G1A3E1S0C0_PSB\\\\0001_G1A3E1S0C0_PSB_001047.json',\n",
       " 'C:\\\\Users\\\\SSAFY\\\\Downloads\\\\015.감성 및 발화 스타일별 음성합성 데이터\\\\01.데이터\\\\1.Training\\\\라벨링데이터\\\\TL1\\\\TL1\\\\1.감정\\\\1.기쁨\\\\0001_G1A3E1S0C0_PSB\\\\0001_G1A3E1S0C0_PSB_000283.json',\n",
       " 'C:\\\\Users\\\\SSAFY\\\\Downloads\\\\015.감성 및 발화 스타일별 음성합성 데이터\\\\01.데이터\\\\1.Training\\\\라벨링데이터\\\\TL1\\\\TL1\\\\1.감정\\\\1.기쁨\\\\0001_G1A3E1S0C0_PSB\\\\0001_G1A3E1S0C0_PSB_001411.json',\n",
       " 'C:\\\\Users\\\\SSAFY\\\\Downloads\\\\015.감성 및 발화 스타일별 음성합성 데이터\\\\01.데이터\\\\1.Training\\\\라벨링데이터\\\\TL1\\\\TL1\\\\1.감정\\\\1.기쁨\\\\0001_G1A3E1S0C0_PSB\\\\0001_G1A3E1S0C0_PSB_001911.json',\n",
       " 'C:\\\\Users\\\\SSAFY\\\\Downloads\\\\015.감성 및 발화 스타일별 음성합성 데이터\\\\01.데이터\\\\1.Training\\\\라벨링데이터\\\\TL1\\\\TL1\\\\1.감정\\\\1.기쁨\\\\0001_G1A3E1S0C0_PSB\\\\0001_G1A3E1S0C0_PSB_001724.json']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def match_label_file(wav_path, base_dir:str, is_train:bool):\n",
    "    wav_tag = 'TS' if is_train else 'VS' \n",
    "    json_tag = 'TL' if is_train else 'VL' \n",
    "    label_path = wav_path.replace(\"원천데이터\", \"라벨링데이터\")\n",
    "    label_path = label_path.replace(f'{wav_tag}1', f'{json_tag}1')\n",
    "    label_path = label_path.replace(\".wav\", \".json\")\n",
    "    return label_path\n",
    "\n",
    "label_path_list = [match_label_file(wav, BASE_DIR, True) for wav in wav_list if os.path.exists(wav)]\n",
    "label_path_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14dbef2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Happy'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "def get_emotion_label(json_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    # pprint(data['화자정보'])\n",
    "    emotion = data['화자정보']['Emotion']\n",
    "    return emotion\n",
    "\n",
    "get_emotion_label(label_path_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c0adea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dict()\n",
    "for label in label_path_list:\n",
    "    emo = get_emotion_label(label)\n",
    "    if temp.get(emo, False): temp[emo] += 1\n",
    "    else: temp[emo] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0a6cfed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Happy': 3300,\n",
       " 'Sad': 3200,\n",
       " 'Angry': 3200,\n",
       " 'Anxious': 3200,\n",
       " 'Hurt': 3200,\n",
       " 'Embarrassed': 3200,\n",
       " 'Neutrality': 3200}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f25ef197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Happy': 0, 'Sad': 1, 'Angry': 2, 'Anxious': 3, 'Hurt': 4, 'Embarrassed': 5, 'Neutrality': 6} {0: 'Happy', 1: 'Sad', 2: 'Angry', 3: 'Anxious', 4: 'Hurt', 5: 'Embarrassed', 6: 'Neutrality'}\n"
     ]
    }
   ],
   "source": [
    "_id = 0\n",
    "label2id = dict()\n",
    "id2label = dict()\n",
    "for emo in temp.keys():\n",
    "    label2id[emo] = _id\n",
    "    id2label[_id] = emo\n",
    "    _id += 1\n",
    "    \n",
    "del _id\n",
    "print(label2id, id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20e9978f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = list(label2id[e] for e in map(get_emotion_label, label_path_list))\n",
    "label_list[::3200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eabb7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_files, test_files, train_labels, test_labels = train_test_split(\n",
    "    wav_list, label_list, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b15cda1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_mfcc(file_path, n_mfcc=40, max_len=300):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    \n",
    "    if mfcc.shape[1] < max_len:\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0,0),(0,pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    return mfcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f4d5e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AudioEmotionDataset(Dataset):\n",
    "    def __init__(self, file_list, label_list):\n",
    "        self.file_list = file_list\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfcc = extract_mfcc(self.file_list[idx])\n",
    "        label = self.label_list[idx]\n",
    "        return torch.tensor(mfcc, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = AudioEmotionDataset(train_files, train_labels)\n",
    "test_dataset = AudioEmotionDataset(test_files, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5eff6481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Emotion1DCNN(nn.Module):\n",
    "    def __init__(self, n_mfcc=40, max_len=300, num_classes=7):\n",
    "        super(Emotion1DCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(n_mfcc, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc1 = nn.Linear(128 * (max_len // 2), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, n_mfcc, max_len)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4cc1db52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "98dce955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.7221\n",
      "Epoch 2, Loss: 1.4904\n",
      "Epoch 3, Loss: 1.3824\n",
      "Epoch 4, Loss: 1.3009\n",
      "Epoch 5, Loss: 1.2176\n",
      "Epoch 6, Loss: 1.1466\n",
      "Epoch 7, Loss: 1.0700\n",
      "Epoch 8, Loss: 1.0156\n",
      "Epoch 9, Loss: 0.9515\n",
      "Epoch 10, Loss: 0.8856\n",
      "Epoch 11, Loss: 0.8362\n",
      "Epoch 12, Loss: 0.7784\n",
      "Epoch 13, Loss: 0.7198\n",
      "Epoch 14, Loss: 0.6739\n",
      "Epoch 15, Loss: 0.6250\n",
      "Epoch 16, Loss: 0.5867\n",
      "Epoch 17, Loss: 0.5386\n",
      "Epoch 18, Loss: 0.5075\n",
      "Epoch 19, Loss: 0.4711\n",
      "Epoch 20, Loss: 0.4400\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Emotion1DCNN(num_classes=7).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for mfccs, labels in train_loader:\n",
    "        mfccs, labels = mfccs.to(device), labels.to(device)\n",
    "        mfccs = mfccs  # (batch, n_mfcc, max_len)\n",
    "        outputs = model(mfccs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b200861c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7423    0.7934    0.7670       639\n",
      "           1     0.6358    0.6916    0.6625       694\n",
      "           2     0.4507    0.5306    0.4874       637\n",
      "           3     0.3595    0.3895    0.3739       611\n",
      "           4     0.3737    0.2900    0.3265       607\n",
      "           5     0.4860    0.4356    0.4594       675\n",
      "           6     0.6760    0.6091    0.6408       637\n",
      "\n",
      "    accuracy                         0.5380      4500\n",
      "   macro avg     0.5320    0.5343    0.5311      4500\n",
      "weighted avg     0.5350    0.5380    0.5345      4500\n",
      "\n",
      "[[507  18  82  14   3   7   8]\n",
      " [ 45 480  14  57  55  11  32]\n",
      " [ 66  33 338  31  22 103  44]\n",
      " [ 13  77  45 238  93 127  18]\n",
      " [ 17  87  53 151 176  53  70]\n",
      " [ 16  18 143 153  37 294  14]\n",
      " [ 19  42  75  18  85  10 388]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true, y_pred = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        preds = model(x.to(device)).argmax(1).cpu().numpy()\n",
    "        y_true.extend(y.numpy()); y_pred.extend(preds)\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7c47f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, audio_path, device):\n",
    "    model.eval()\n",
    "    mfcc = extract_mfcc(audio_path)                       # (n_mfcc, max_len)\n",
    "    x = np.expand_dims(mfcc, 0)                  # (1, n_mfcc, max_len)\n",
    "    x = torch.tensor(x, dtype=torch.float32)     # numpy → tensor\n",
    "    x = x.to(device)                             # (1, C, L) on GPU/CPU\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(x)                           # (1, num_classes)\n",
    "        pred = torch.argmax(out, dim=1).item()\n",
    "    return id2label[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "4ae09b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.57142857142857%\n"
     ]
    }
   ],
   "source": [
    "val_list = find_wav_files(BASE_DIR, False, 1)\n",
    "ans = {\n",
    "    '기쁨': 'Happy',\n",
    "    '슬픔': 'Sad',\n",
    "    '분노': 'Angry',\n",
    "    '당황': 'Embarrassed',\n",
    "    '상처': 'Hurt',\n",
    "    '중립': 'Neutrality',\n",
    "    '불안': 'Anxious'\n",
    "}\n",
    "\n",
    "res = 0\n",
    "\n",
    "for wav in val_list:\n",
    "    emo = wav.split(\"1.감정\\\\\")[1].split(\"\\\\\")[0][2:]\n",
    "    inferred = infer(model, wav, device)\n",
    "    if inferred == ans[emo]: res += 1\n",
    "res = res / len(val_list) * 100\n",
    "print(f'{res}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ad5fb97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.538\n",
      "Macro F1: 0.531078086798181\n",
      "Macro Recall: 0.5342598295980111\n",
      "Macro Precision: 0.5319771190610262\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7423    0.7934    0.7670       639\n",
      "           1     0.6358    0.6916    0.6625       694\n",
      "           2     0.4507    0.5306    0.4874       637\n",
      "           3     0.3595    0.3895    0.3739       611\n",
      "           4     0.3737    0.2900    0.3265       607\n",
      "           5     0.4860    0.4356    0.4594       675\n",
      "           6     0.6760    0.6091    0.6408       637\n",
      "\n",
      "    accuracy                         0.5380      4500\n",
      "   macro avg     0.5320    0.5343    0.5311      4500\n",
      "weighted avg     0.5350    0.5380    0.5345      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, recall_score, precision_score, accuracy_score\n",
    "\n",
    "# ... (모델 학습 및 평가 루프는 동일)\n",
    "\n",
    "# 1. 예측값과 실제값 수집\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "# 2. 주요 지표 출력\n",
    "print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "print(\"Macro F1:\", f1_score(all_labels, all_preds, average='macro'))\n",
    "print(\"Macro Recall:\", recall_score(all_labels, all_preds, average='macro'))\n",
    "print(\"Macro Precision:\", precision_score(all_labels, all_preds, average='macro'))\n",
    "\n",
    "# 3. 전체 리포트 (클래스별 지표 포함)\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d81e833a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m.save(model.state_dict(), \u001b[33m\"\u001b[39m\u001b[33mcnn_1d_model.pth\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"cnn_1d_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
