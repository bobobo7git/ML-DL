{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34bb75ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "import torchaudio\n",
    "import torch\n",
    "import librosa\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a386ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"openai/whisper-large-v3\"\n",
    "EMOTION_LABELS = [\"happiness\", \"angry\", \"disgust\", \"fear\", \"neutral\", \"sadness\", \"surprise\"]\n",
    "LABEL2ID = {emo: i for i, emo in enumerate(EMOTION_LABELS)}\n",
    "ID2LABEL = {i: emo for emo, i in LABEL2ID.items()}\n",
    "TARGET_SR = 16000\n",
    "AUDIO_DIR = 'samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8015b71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>발화문</th>\n",
       "      <th>상황</th>\n",
       "      <th>1번 감정</th>\n",
       "      <th>1번 감정세기</th>\n",
       "      <th>2번 감정</th>\n",
       "      <th>2번 감정세기</th>\n",
       "      <th>3번 감정</th>\n",
       "      <th>3번 감정세기</th>\n",
       "      <th>4번 감정</th>\n",
       "      <th>4번 감정세기</th>\n",
       "      <th>5번 감정</th>\n",
       "      <th>5번 감정세기</th>\n",
       "      <th>나이</th>\n",
       "      <th>성별</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wav_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5e258fd1305bcf3ad153a6a4</th>\n",
       "      <td>어, 청소 니가 대신 해 줘!</td>\n",
       "      <td>anger</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>Angry</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>Angry</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5e258fe2305bcf3ad153a6a5</th>\n",
       "      <td>둘 다 청소 하기 싫어. 귀찮아.</td>\n",
       "      <td>anger</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>Angry</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>Angry</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5e258ff5305bcf3ad153a6a6</th>\n",
       "      <td>둘 다 하기 싫어서 화내.</td>\n",
       "      <td>anger</td>\n",
       "      <td>Angry</td>\n",
       "      <td>1</td>\n",
       "      <td>Angry</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>Angry</td>\n",
       "      <td>1</td>\n",
       "      <td>Angry</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5e25902f305bcf3ad153a6a9</th>\n",
       "      <td>그럼 방세는 어떡해.</td>\n",
       "      <td>anger</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>1</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>1</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>1</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>1</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5e27f90b5807b852d9e0157b</th>\n",
       "      <td>권태긴줄 알았는데 다른 사람이 생겼나보더라고.</td>\n",
       "      <td>sad</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>1</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>1</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>1</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>2</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                발화문     상황    1번 감정  1번 감정세기  \\\n",
       "wav_id                                                                         \n",
       "5e258fd1305bcf3ad153a6a4           어, 청소 니가 대신 해 줘!  anger  Neutral        0   \n",
       "5e258fe2305bcf3ad153a6a5         둘 다 청소 하기 싫어. 귀찮아.  anger  Neutral        0   \n",
       "5e258ff5305bcf3ad153a6a6             둘 다 하기 싫어서 화내.  anger    Angry        1   \n",
       "5e25902f305bcf3ad153a6a9                그럼 방세는 어떡해.  anger  Sadness        1   \n",
       "5e27f90b5807b852d9e0157b  권태긴줄 알았는데 다른 사람이 생겼나보더라고.    sad  Sadness        1   \n",
       "\n",
       "                            2번 감정  2번 감정세기    3번 감정  3번 감정세기    4번 감정  \\\n",
       "wav_id                                                                  \n",
       "5e258fd1305bcf3ad153a6a4    Angry        1  Neutral        0  Neutral   \n",
       "5e258fe2305bcf3ad153a6a5    Angry        1  Neutral        0  Neutral   \n",
       "5e258ff5305bcf3ad153a6a6    Angry        1  Neutral        0    Angry   \n",
       "5e25902f305bcf3ad153a6a9  Sadness        1  Sadness        1  Sadness   \n",
       "5e27f90b5807b852d9e0157b  Sadness        1  Sadness        1  Sadness   \n",
       "\n",
       "                          4번 감정세기    5번 감정  5번 감정세기  나이    성별  \n",
       "wav_id                                                         \n",
       "5e258fd1305bcf3ad153a6a4        0    Angry        1  27  male  \n",
       "5e258fe2305bcf3ad153a6a5        0    Angry        1  27  male  \n",
       "5e258ff5305bcf3ad153a6a6        1    Angry        1  27  male  \n",
       "5e25902f305bcf3ad153a6a9        1  Sadness        1  27  male  \n",
       "5e27f90b5807b852d9e0157b        2  Sadness        1  32  male  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df_01 = pd.read_csv(\"4th.csv\", encoding='cp949').set_index(\"wav_id\")\n",
    "labels_df_02 = pd.read_csv(\"5th_1st.csv\", encoding='cp949').set_index(\"wav_id\")\n",
    "labels_df_03 = pd.read_csv(\"5th_2nd.csv\", encoding='cp949').set_index(\"wav_id\")\n",
    "\n",
    "# 4차 + 5차_1차 + 5차_2차\n",
    "df = pd.concat([labels_df_01, labels_df_02, labels_df_03])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1fed61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_final_emotion(row):\n",
    "    emo_vals = { emo:0 for emo in EMOTION_LABELS}\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        data_emo = row.get(f'{i}번 감정').lower()\n",
    "        data_emo_val = row.get(f'{i}번 감정세기')\n",
    "        if data_emo == 'neutral': emo_vals['neutral'] += 1\n",
    "        else: emo_vals[data_emo] += data_emo_val\n",
    "    \n",
    "    # 중립이 4개 이상인 경우 중립 리턴\n",
    "    if emo_vals['neutral'] > 3:\n",
    "        return 'neutral'\n",
    "    \n",
    "    # 중립이 3개 이하면 가중 최빈값 리턴\n",
    "    max_val = max(emo_vals.values())\n",
    "    for emo, val in emo_vals.items():\n",
    "        if max_val == val:\n",
    "            return emo\n",
    "    \n",
    "    return 'neutral'\n",
    "\n",
    "df['final_emotion'] = df.apply(tag_final_emotion, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9a71ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_emotion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wav_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5e258fd1305bcf3ad153a6a4</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5e258fe2305bcf3ad153a6a5</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5e258ff5305bcf3ad153a6a6</th>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5e25902f305bcf3ad153a6a9</th>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5e27f90b5807b852d9e0157b</th>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         final_emotion\n",
       "wav_id                                \n",
       "5e258fd1305bcf3ad153a6a4       neutral\n",
       "5e258fe2305bcf3ad153a6a5       neutral\n",
       "5e258ff5305bcf3ad153a6a6         angry\n",
       "5e25902f305bcf3ad153a6a9       sadness\n",
       "5e27f90b5807b852d9e0157b       sadness"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "META_COLUMS = [\n",
    "    \"발화문\", \"상황\", \"1번 감정\", \"1번 감정세기\", \"2번 감정\", \"2번 감정세기\",\n",
    "    \"3번 감정\", \"3번 감정세기\", \"4번 감정\", \"4번 감정세기\", \"5번 감정\", \"5번 감정세기\",\n",
    "    \"나이\", \"성별\"\n",
    "]\n",
    "\n",
    "df = df.drop(columns=META_COLUMS)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15c37ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_emotion\n",
       "sadness          16882\n",
       "angry             8650\n",
       "neutral           7030\n",
       "happiness         4506\n",
       "fear              3150\n",
       "disgust           2708\n",
       "surprise          1065\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e5dffb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_emotion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wav_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5f5cc76e2e23c7161accd04b</th>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5f600eb454b2361621284a68</th>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5f67f3c99e04b149046cb7bb</th>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5f6538aef8fac448cc0a57fb</th>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5f7866dbf8fac448cc0a63d5</th>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         final_emotion\n",
       "wav_id                                \n",
       "5f5cc76e2e23c7161accd04b     happiness\n",
       "5f600eb454b2361621284a68     happiness\n",
       "5f67f3c99e04b149046cb7bb     happiness\n",
       "5f6538aef8fac448cc0a57fb     happiness\n",
       "5f7866dbf8fac448cc0a63d5     happiness"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def under_sampling(max_count_per_class):\n",
    "    df_balanced = pd.concat([\n",
    "        df[df['final_emotion'] == emo].sample(n=max_count_per_class, random_state=42, replace=False)\n",
    "        if len(df[df['final_emotion'] == emo]) > max_count_per_class else df[df['final_emotion'] == emo]\n",
    "        for emo in EMOTION_LABELS\n",
    "    ])\n",
    "    return df_balanced\n",
    "\n",
    "df_balanced = under_sampling(1000)\n",
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd9c280a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_emotion\n",
       "angry            1000\n",
       "disgust          1000\n",
       "fear             1000\n",
       "happiness        1000\n",
       "neutral          1000\n",
       "sadness          1000\n",
       "surprise         1000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "945169e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_emotion</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wav_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5f5cc76e2e23c7161accd04b</th>\n",
       "      <td>happiness</td>\n",
       "      <td>samples\\5f5cc76e2e23c7161accd04b.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5f600eb454b2361621284a68</th>\n",
       "      <td>happiness</td>\n",
       "      <td>samples\\5f600eb454b2361621284a68.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5f67f3c99e04b149046cb7bb</th>\n",
       "      <td>happiness</td>\n",
       "      <td>samples\\5f67f3c99e04b149046cb7bb.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5f6538aef8fac448cc0a57fb</th>\n",
       "      <td>happiness</td>\n",
       "      <td>samples\\5f6538aef8fac448cc0a57fb.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5f7866dbf8fac448cc0a63d5</th>\n",
       "      <td>happiness</td>\n",
       "      <td>samples\\5f7866dbf8fac448cc0a63d5.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         final_emotion                                  path\n",
       "wav_id                                                                      \n",
       "5f5cc76e2e23c7161accd04b     happiness  samples\\5f5cc76e2e23c7161accd04b.wav\n",
       "5f600eb454b2361621284a68     happiness  samples\\5f600eb454b2361621284a68.wav\n",
       "5f67f3c99e04b149046cb7bb     happiness  samples\\5f67f3c99e04b149046cb7bb.wav\n",
       "5f6538aef8fac448cc0a57fb     happiness  samples\\5f6538aef8fac448cc0a57fb.wav\n",
       "5f7866dbf8fac448cc0a63d5     happiness  samples\\5f7866dbf8fac448cc0a63d5.wav"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def join_path(row):\n",
    "    wav_id = row.name\n",
    "    path_col = os.path.join(AUDIO_DIR, wav_id+'.wav')\n",
    "    if (os.path.exists(path_col)):\n",
    "        return path_col\n",
    "    return None\n",
    "\n",
    "df_balanced['path'] = df_balanced.apply(join_path, axis=1)\n",
    "df_balanced = df_balanced.dropna(subset=['path'])\n",
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95663e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_emotion</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wav_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5f87c727111dfd48d40fe30d</th>\n",
       "      <td>surprise</td>\n",
       "      <td>samples\\5f87c727111dfd48d40fe30d.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5e4255b29306c7039ddccbf0</th>\n",
       "      <td>disgust</td>\n",
       "      <td>samples\\5e4255b29306c7039ddccbf0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5fbdb2a34c55eb78bd7ceb29</th>\n",
       "      <td>disgust</td>\n",
       "      <td>samples\\5fbdb2a34c55eb78bd7ceb29.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5fbb63f7576e9378b67acb29</th>\n",
       "      <td>happiness</td>\n",
       "      <td>samples\\5fbb63f7576e9378b67acb29.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5f697fa89e04b149046cb954</th>\n",
       "      <td>neutral</td>\n",
       "      <td>samples\\5f697fa89e04b149046cb954.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         final_emotion                                  path\n",
       "wav_id                                                                      \n",
       "5f87c727111dfd48d40fe30d      surprise  samples\\5f87c727111dfd48d40fe30d.wav\n",
       "5e4255b29306c7039ddccbf0       disgust  samples\\5e4255b29306c7039ddccbf0.wav\n",
       "5fbdb2a34c55eb78bd7ceb29       disgust  samples\\5fbdb2a34c55eb78bd7ceb29.wav\n",
       "5fbb63f7576e9378b67acb29     happiness  samples\\5fbb63f7576e9378b67acb29.wav\n",
       "5f697fa89e04b149046cb954       neutral  samples\\5f697fa89e04b149046cb954.wav"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = df_balanced.sample(frac=0.8, random_state=42)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8007b135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_emotion</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wav_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5f6538aef8fac448cc0a57fb</th>\n",
       "      <td>happiness</td>\n",
       "      <td>samples\\5f6538aef8fac448cc0a57fb.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5f7866dbf8fac448cc0a63d5</th>\n",
       "      <td>happiness</td>\n",
       "      <td>samples\\5f7866dbf8fac448cc0a63d5.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5f68d6da9e04b149046cb8de</th>\n",
       "      <td>happiness</td>\n",
       "      <td>samples\\5f68d6da9e04b149046cb8de.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5fbca3bb44697678c497bafe</th>\n",
       "      <td>happiness</td>\n",
       "      <td>samples\\5fbca3bb44697678c497bafe.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5f5e1d4754b2361621284912</th>\n",
       "      <td>happiness</td>\n",
       "      <td>samples\\5f5e1d4754b2361621284912.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         final_emotion                                  path\n",
       "wav_id                                                                      \n",
       "5f6538aef8fac448cc0a57fb     happiness  samples\\5f6538aef8fac448cc0a57fb.wav\n",
       "5f7866dbf8fac448cc0a63d5     happiness  samples\\5f7866dbf8fac448cc0a63d5.wav\n",
       "5f68d6da9e04b149046cb8de     happiness  samples\\5f68d6da9e04b149046cb8de.wav\n",
       "5fbca3bb44697678c497bafe     happiness  samples\\5fbca3bb44697678c497bafe.wav\n",
       "5f5e1d4754b2361621284912     happiness  samples\\5f5e1d4754b2361621284912.wav"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = df_balanced.drop(train_data.index)\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9930cde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_emotion\n",
       "happiness    815\n",
       "angry        813\n",
       "surprise     800\n",
       "neutral      796\n",
       "disgust      794\n",
       "fear         792\n",
       "sadness      789\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['final_emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5a02b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_emotion\n",
       "sadness      211\n",
       "fear         208\n",
       "disgust      206\n",
       "neutral      204\n",
       "surprise     200\n",
       "angry        186\n",
       "happiness    185\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['final_emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aefe378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Dataset.from_pandas(train_data)\n",
    "X_test = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d82dbc",
   "metadata": {},
   "source": [
    "## audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c802e326",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5599/5599 [00:29<00:00, 190.09 examples/s]\n",
      "Map: 100%|██████████| 1400/1400 [00:05<00:00, 261.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def read_audio(path):\n",
    "    array, sampling_rate = librosa.load(path, sr=None)\n",
    "    return array, sampling_rate\n",
    "\n",
    "def convert_example(example):\n",
    "    audio_path = example['path']\n",
    "    array, sampling_rate = read_audio(audio_path)\n",
    "    return {\n",
    "        'audio': {\n",
    "            'path': audio_path,\n",
    "            'array': array,\n",
    "            'sampling_rate': sampling_rate\n",
    "        },\n",
    "        'labels': LABEL2ID[example['final_emotion']]\n",
    "    }\n",
    "\n",
    "def convert_dataset(dataset):\n",
    "    converted_examples = []\n",
    "    for example in dataset:\n",
    "        converted_example = convert_example(example)\n",
    "        converted_examples.append(converted_example)\n",
    "    return Dataset.from_dict(converted_examples)\n",
    "\n",
    "converted_train = X_train.map(convert_example)\n",
    "converted_test = X_test.map(convert_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "433f30b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['final_emotion', 'path', 'wav_id', 'audio', 'labels'],\n",
       "     num_rows: 5599\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['final_emotion', 'path', 'wav_id', 'audio', 'labels'],\n",
       "     num_rows: 1400\n",
       " }))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_train, converted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a68fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_train = converted_train.remove_columns([\"path\", \"final_emotion\", \"wav_id\"])\n",
    "converted_test = converted_test.remove_columns([\"path\", \"final_emotion\", \"wav_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d78dc73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['audio', 'labels'],\n",
       "     num_rows: 5599\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['audio', 'labels'],\n",
       "     num_rows: 1400\n",
       " }))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_train, converted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26b1b94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (13/13 shards): 100%|██████████| 5599/5599 [00:08<00:00, 656.14 examples/s]\n",
      "Saving the dataset (4/4 shards): 100%|██████████| 1400/1400 [00:01<00:00, 722.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "converted_train.save_to_disk('/content/converted_train')\n",
    "converted_test.save_to_disk('/content/converted_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2c96b0",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3e2229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai/whisper-tiny\"\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    model_id, do_normalize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e7b6208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate = feature_extractor.sampling_rate\n",
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "facb0969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -2.81e-05, Variance: 7.97e-06\n"
     ]
    }
   ],
   "source": [
    "sample = converted_train[0][\"audio\"]\n",
    "\n",
    "print(f\"Mean: {np.mean(sample['array']):.3}, Variance: {np.var(sample['array']):.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69205ad0",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32de8128",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_duration = 30.0\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=int(feature_extractor.sampling_rate * max_duration),\n",
    "        truncation=True,\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26269b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5599/5599 [10:24<00:00,  8.96 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_features'],\n",
       "    num_rows: 5599\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_encoded_train = converted_train.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=[\"audio\"],\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "data_encoded_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f95cd5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1400/1400 [02:34<00:00,  9.04 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_features'],\n",
       "    num_rows: 1400\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_encoded_test = converted_test.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=[\"audio\"],\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "data_encoded_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ffd794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(ID2LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa560cbe",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0ccda30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at openai/whisper-tiny and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=num_labels,\n",
    "    label2id=LABEL2ID,\n",
    "    id2label=ID2LABEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e7307",
   "metadata": {},
   "source": [
    "## training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "717c2f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\transformers\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__\n",
    "print(transformers.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91859445",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1  # Updated batch size\n",
    "gradient_accumulation_steps = 16  # Updated gradient accumulation steps\n",
    "num_train_epochs = 5  # Updated number of epochs\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"speech-emotion-recognition-with-openai-whisper-large-v3\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    disable_tqdm=False,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79bcdbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08)\n",
    "lr_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=len(train_data) * training_args.num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f92c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from IPython.display import Audio\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    labels = eval_pred.label_ids\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22e42055",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5346fa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SSAFY\\AppData\\Local\\Temp\\ipykernel_37176\\3577341253.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer, BitsAndBytesConfig, EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=data_encoded_train,\n",
    "    eval_dataset=data_encoded_test,\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, lr_scheduler),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "168476d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1745' max='1745' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1745/1745 2:17:15, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.793000</td>\n",
       "      <td>1.841297</td>\n",
       "      <td>0.265714</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.265714</td>\n",
       "      <td>0.210689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.753900</td>\n",
       "      <td>1.743399</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.317899</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.302896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.605800</td>\n",
       "      <td>1.705263</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.367289</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.320888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.109800</td>\n",
       "      <td>1.636867</td>\n",
       "      <td>0.380714</td>\n",
       "      <td>0.377440</td>\n",
       "      <td>0.380714</td>\n",
       "      <td>0.367533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "c:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "c:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "c:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "c:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1745, training_loss=1.6234865314297826, metrics={'train_runtime': 8239.2573, 'train_samples_per_second': 3.398, 'train_steps_per_second': 0.212, 'total_flos': 3.1084884516672e+17, 'train_loss': 1.6234865314297826, 'epoch': 4.985890337560279})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33917ef5",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "435b33eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Desktop\\_fpjt\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./whisper-tiny-korean-emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de1fbaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./whisper-tiny-korean-emotion\"\n",
    "model = AutoModelForAudioClassification.from_pretrained(model_path)\n",
    "extractor = AutoFeatureExtractor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58cba739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'happiness',\n",
       " 1: 'angry',\n",
       " 2: 'disgust',\n",
       " 3: 'fear',\n",
       " 4: 'neutral',\n",
       " 5: 'sadness',\n",
       " 6: 'surprise'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID2LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61c3b553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_path, feature_extractor, max_duration=30.0):\n",
    "    audio_array, sampling_rate = librosa.load(audio_path, sr=feature_extractor.sampling_rate)\n",
    "\n",
    "    max_length = int(feature_extractor.sampling_rate * max_duration)\n",
    "    if len(audio_array) > max_length:\n",
    "        audio_array = audio_array[:max_length]\n",
    "    else:\n",
    "        audio_array = np.pad(audio_array, (0, max_length - len(audio_array)))\n",
    "\n",
    "    inputs = feature_extractor(\n",
    "        audio_array,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c12b172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(audio_path, model, feature_extractor, id2label, max_duration=30.0):\n",
    "    inputs = preprocess_audio(audio_path, feature_extractor, max_duration)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predicted_id = torch.argmax(logits, dim=-1).item()\n",
    "    predicted_label = id2label[predicted_id]\n",
    "\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1424ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: angry.mp3 | Predicted Emotion: neutral\n",
      "file: angry2.mp3 | Predicted Emotion: angry\n",
      "file: hap.mp3 | Predicted Emotion: neutral\n",
      "file: sad2.mp3 | Predicted Emotion: surprise\n",
      "file: sad3.mp3 | Predicted Emotion: surprise\n",
      "file: real_happy.wav | Predicted Emotion: neutral\n",
      "file: sad4.mp3 | Predicted Emotion: neutral\n",
      "file: sad5.mp3 | Predicted Emotion: neutral\n",
      "file: annie_happy.mp3 | Predicted Emotion: angry\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for path in [\"angry.mp3\", \"angry2.mp3\", \"hap.mp3\", \"sad2.mp3\", \"sad3.mp3\", \"real_happy.wav\", \"sad4.mp3\", \"sad5.mp3\", \"annie_happy.mp3\"]:\n",
    "    predicted_emotion = predict_emotion(path, model, extractor, ID2LABEL)\n",
    "    print(f\"file: {path} | Predicted Emotion: {predicted_emotion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc2e2c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n"
     ]
    }
   ],
   "source": [
    "print(predict_emotion(\"hap2.wav\",model,extractor, ID2LABEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda2c733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
